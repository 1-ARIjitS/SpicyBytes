from pyspark.sql.types import StringType, FloatType, IntegerType
import random
# import datetime
import logging 
import os 
import configparser
import json
from pyspark.sql import SparkSession
from datetime import datetime, timedelta
from pyspark.sql.functions import udf, monotonically_increasing_id, col, regexp_replace, lit,trim


os.environ["PYSPARK_PYTHON"] = "/home/pce/anaconda3/envs/spark_env/bin/python3.11"
os.environ["PYSPARK_DRIVER_PYTHON"] = "/home/pce/anaconda3/envs/spark_env/bin/python3.11"

# Configure logging
logging.basicConfig(level=logging.INFO)  # Set log level to INFO

# Create logger object
logger = logging.getLogger()

# Get base directory
root_dir = os.path.abspath(os.path.join(os.getcwd()))

# Specify the path to config file
config_file_path = os.path.join(root_dir, "config.ini")
config = configparser.ConfigParser()
config.read(config_file_path)

config_file_path_json = os.path.join(root_dir, "config.json")
with open(config_file_path_json) as f:
    config_json = json.load(f)
    
    

gcs_config = config["GCS"]["credentials_path"]
raw_bucket_name = config["GCS"]["raw_bucket_name"]
formatted_bucket_name = config["GCS"]["formatted_bucket_name"]
exploitation_bucket_name = config["GCS"]["exploitation_bucket_name"]
project_id = config['BIGQUERY']['project_id']
dataset_id = config['BIGQUERY']['dataset_id']


# Initialize the Spark session
spark = SparkSession.builder \
    .appName("customer_2_customer_facts") \
      .config("spark.driver.host", "127.0.0.1") \
      .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
      .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
      .config('spark.jars', 'spark-bigquery-with-dependencies_2.12-0.27.0.jar') \
      .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
      .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", gcs_config) \
      .config("temporaryGcsBucket", raw_bucket_name) \
      .config("parentProject", project_id) \
      .config("project", project_id) \
      .getOrCreate()


# Load the DataFrames
customer_purchase_path = "/home/pce/Documents/VBP_Joint_Project-main/data/customer_purchase"
supermarket_products_path = "/home/pce/Documents/VBP_Joint_Project-main/data/supermarket_products"
supermarket_location_path = "/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/dim_supermarket.parquet"
products_parquet_path = "/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/dim_product.parquet"
date_parquet_path = "/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/dim_date.parquet"
customer_parquet_path = "/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/dim_customer.parquet"

customer_purchase_df = spark.read.parquet(customer_purchase_path)
supermarket_products_df = spark.read.parquet(supermarket_products_path)
supermarket_location_df = spark.read.parquet(supermarket_location_path)
product_df = spark.read.parquet(products_parquet_path)
date_df = spark.read.parquet(date_parquet_path)
customer_df = spark.read.parquet(customer_parquet_path)

# Filter store_ids to ensure they only include IDs present in supermarket_location_df
valid_store_ids = [row['store_id'] for row in supermarket_products_df.select("store_id").distinct().collect()
                   if row['store_id'] in [row['supermarket_id'] for row in supermarket_location_df.select("supermarket_id").distinct().collect()]]

# Define a UDF to randomly assign a valid store_id
def get_random_store_id():
    return random.choice(valid_store_ids)

random_store_id_udf = udf(get_random_store_id, StringType())

# Define a UDF to generate random purchase dates within the past year
def get_random_purchase_date():
    start_date = datetime.now() - timedelta(days=365)
    random_date = start_date + timedelta(days=random.randint(0, 365))
    return random_date.strftime('%Y-%m-%d')

random_purchase_date_udf = udf(get_random_purchase_date, StringType())

# Define a UDF to generate random quantities between 1 and 10
def get_random_quantity():
    return random.randint(1, 10)

random_quantity_udf = udf(get_random_quantity, IntegerType())

# Define a UDF to calculate dynamic price based on unit_price and proximity to expiry_date
def calculate_dynamic_price(unit_price, purchase_date, expiry_date):
    if unit_price is None or purchase_date is None or expiry_date is None:
        return unit_price
    purchase_date = datetime.strptime(purchase_date, '%Y-%m-%d')
    expiry_date = datetime.strptime(expiry_date, '%d %b %Y')
    days_to_expiry = (expiry_date - purchase_date).days
    discount_factor = max(0.1, min(1, days_to_expiry / 365))
    return float(unit_price) * discount_factor

calculate_dynamic_price_udf = udf(calculate_dynamic_price, FloatType())

# Join customer_purchase_df with supermarket_products_df to get expiry_date
joined_df = customer_purchase_df.join(
    supermarket_products_df.select("product_name", "expiry_date"),
    on="product_name",
    how="left"
)

# Add the random_purchase_date, random_quantity, store_id, and dynamic_price columns
joined_df = joined_df.withColumn("purchase_date", random_purchase_date_udf())
joined_df = joined_df.withColumn("quantity", random_quantity_udf())
joined_df = joined_df.withColumn("store_id", random_store_id_udf())
joined_df = joined_df.withColumn("dynamic_price", calculate_dynamic_price_udf(col("unit_price"), col("purchase_date"), col("expiry_date")))

# Ensure both columns have the same data type, remove leading/trailing spaces, and clean hidden characters
clean_store_id_udf = udf(lambda x: ''.join(c for c in x if c.isprintable()), StringType())
joined_df = joined_df.withColumn("store_id", trim(clean_store_id_udf(col("store_id"))).cast("string"))
supermarket_location_df = supermarket_location_df.withColumn("supermarket_id", trim(clean_store_id_udf(col("supermarket_id"))).cast("string"))

# Display some samples to debug
print("Sample store_ids from joined_df:")
joined_df.select("store_id").distinct().show(5, truncate=False)

print("Sample supermarket_ids from supermarket_location_df:")
supermarket_location_df.select("supermarket_id").distinct().show(5, truncate=False)

# Perform an inner join to check for matches
matches_df = joined_df.join(
    supermarket_location_df.select("supermarket_id"),
    joined_df["store_id"] == supermarket_location_df["supermarket_id"],
    "inner"
)

# Show the number of matches
match_count = matches_df.count()
print(f"Number of matches: {match_count}")

# Display some matching rows if there are any
if match_count > 0:
    matches_df.show()
else:
    print("No matches found.")

# Join with supermarket_location_df to get location_id
joined_df = joined_df.join(
    supermarket_location_df.select("supermarket_id", "location_id"),
    joined_df["store_id"] == supermarket_location_df["supermarket_id"],
    "left"
)

# Join for product information
joined_df = joined_df.join(
    product_df.select("product_id", "product_name"),
    joined_df["product_name"] == product_df["product_name"],
    "left"
).drop(product_df["product_name"])

# Join for selling date information
joined_df = joined_df.join(
    date_df.select("date_id", "date").withColumnRenamed("date_id", "purchase_date_id"),
    joined_df["purchase_date"] == date_df["date"],
    "left"
).drop(date_df["date"])

# Join for buyer location
joined_df = joined_df.join(
    customer_df.select("customer_id", "location_id").withColumnRenamed("location_id", "buyer_location_id"),
    joined_df["customer_id"] == customer_df["customer_id"],
    "left"
).drop(customer_df["customer_id"])
joined_df = joined_df.withColumn("created_on",lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))  # Add created_on


# Select the required columns
fact_business_cust_purchase_df = joined_df.select(
    "store_id",
    "customer_id",
    "location_id",
    "buyer_location_id",
    "product_id",
    "purchase_date_id",
    "unit_price",
    "quantity",
    "dynamic_price",
    "created_on"
)

# Show the resulting DataFrame
fact_business_cust_purchase_df.show()
fact_business_cust_purchase_df.printSchema()

# Select rows where location_id is not null
fact_business_cust_purchase_df = fact_business_cust_purchase_df.filter(col("location_id").isNotNull())

# Show the resulting DataFrame
fact_business_cust_purchase_df.show()

fact_business_cust_purchase_df.write \
    .format('bigquery') \
    .option('table', f'{project_id}:{dataset_id}.fact_business_cust_purchase') \
    .mode('overwrite') \
    .save()

spark.stop()