import logging 
import os 
import configparser
import json
from pyspark.sql import SparkSession
from datetime import datetime
from pyspark.sql.functions import udf, monotonically_increasing_id, col, regexp_replace, lit, first

# Configure logging
logging.basicConfig(level=logging.INFO)  # Set log level to INFO

# Create logger object
logger = logging.getLogger()

# Get base directory
root_dir = os.path.abspath(os.path.join(os.getcwd()))

# Specify the path to config file
config_file_path = os.path.join(root_dir, "config.ini")
config = configparser.ConfigParser()
config.read(config_file_path)

config_file_path_json = os.path.join(root_dir, "config.json")
with open(config_file_path_json) as f:
    config_json = json.load(f)


if __name__ == "__main__":
    gcs_config = config["GCS"]["credentials_path"]
    raw_bucket_name = config["GCS"]["raw_bucket_name"]
    formatted_bucket_name = config["GCS"]["formatted_bucket_name"]
    exploitation_bucket_name = config["GCS"]["exploitation_bucket_name"]

    spark = SparkSession.builder \
        .appName("Supermarket Dimension table creation") \
        .config("spark.driver.host", "127.0.0.1") \
        .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
        .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
        .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
        .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", gcs_config) \
        .getOrCreate()
   
    logger.info('-----------------------------------------------------')
    logger.info("Creating supermarket dimension table")

     # Read the Parquet file into a DataFrame from GCS Bucket
    supermarket_df = spark.read.parquet('gs://'+formatted_bucket_name+'/establishments_catalonia*.parquet')
    supermarket_df = supermarket_df.dropDuplicates()

    supermarket_df = supermarket_df.withColumnRenamed("id","supermarket_id")\
                                    .withColumnRenamed("commercial_name","supermarket_name")
    supermarket_df = supermarket_df.groupBy("supermarket_id").agg(first("supermarket_name").alias("supermarket_name"),
                                                                  first("full_address").alias("full_address"),
                                                                  first("UTMx").alias("UTMx"),
                                                                  first("UTMy").alias("UTMy"),
                                                                  first("latitude").alias("latitude"),
                                                                  first("longitude").alias("longitude"))
    
    loc_df = supermarket_df.select('full_address', "UTMx", "UTMy", "latitude", "longitude")
    loc_df = loc_df.withColumn("location_id", monotonically_increasing_id()+7000) # To generate supermarket_id starting from id=7000

    sp_loc_df = supermarket_df.join(loc_df, ['full_address', "UTMx", "UTMy", "latitude", "longitude"])

    dim_supermarket = sp_loc_df.select("supermarket_id", "supermarket_name", "location_id")
    
    dim_sp_location = sp_loc_df.select("location_id", "full_address", "UTMx", "UTMy", "latitude", "longitude")
    dim_sp_location = dim_sp_location.dropDuplicates()

    # Add created_on
    dim_supermarket = dim_supermarket.withColumn("created_on",lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))
    dim_sp_location = dim_sp_location.withColumn("created_on",lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))

    dim_supermarket.printSchema()
    dim_sp_location.printSchema()

    # supermarket_df.write.mode('overwrite').parquet(f'gs://{formatted_bucket_name}/dim_supermarket_'+datetime.now().strftime("%Y%m%d%H%M%S")+'.parquet')