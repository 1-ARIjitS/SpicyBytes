import logging 
import os 
import configparser
import json
from pyspark.sql import SparkSession
from datetime import datetime
from pyspark.sql.functions import udf, monotonically_increasing_id, col, regexp_replace, lit

# Configure logging
logging.basicConfig(level=logging.INFO)  # Set log level to INFO

# Create logger object
logger = logging.getLogger()

# Get base directory
root_dir = os.path.abspath(os.path.join(os.getcwd()))

# Specify the path to config file
config_file_path = os.path.join(root_dir, "config.ini")
config = configparser.ConfigParser()
config.read(config_file_path)

config_file_path_json = os.path.join(root_dir, "config.json")
with open(config_file_path_json) as f:
    config_json = json.load(f)
    
    

gcs_config = config["GCS"]["credentials_path"]
raw_bucket_name = config["GCS"]["raw_bucket_name"]
formatted_bucket_name = config["GCS"]["formatted_bucket_name"]
exploitation_bucket_name = config["GCS"]["exploitation_bucket_name"]
project_id = config['BIGQUERY']['project_id']
dataset_id = config['BIGQUERY']['dataset_id']


# Initialize the Spark session
spark = SparkSession.builder \
    .appName("customer_2_customer_facts") \
      .config("spark.driver.host", "127.0.0.1") \
      .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
      .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
      .config('spark.jars', '/home/pce/Documents/VBP_Joint_Project-main/spark-bigquery-with-dependencies_2.12-0.27.0.jar') \
      .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
      .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", gcs_config) \
      .config("temporaryGcsBucket", raw_bucket_name) \
      .config("parentProject", project_id) \
      .config("project", project_id) \
      .getOrCreate()

# Load the main DataFrame
main_parquet_path = "/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/platform_customer_pricing_data_output"
main_df = spark.read.parquet(main_parquet_path)

# Load the dimension tables
location_parquet_path = "/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/dim_customer_location.parquet"
customer_parquet_path = "/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/dim_customer.parquet"
products_parquet_path = "/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/dim_product.parquet"
date_parquet_path = "/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/dim_date.parquet"

location_df = spark.read.parquet(location_parquet_path)
customer_df = spark.read.parquet(customer_parquet_path)
product_df = spark.read.parquet(products_parquet_path)
date_df = spark.read.parquet(date_parquet_path)

# Join for buyer location
joined_df = main_df.join(
    customer_df.select("customer_id", "location_id").withColumnRenamed("location_id", "buyer_location_id"),
    main_df["buying_customer_id"] == customer_df["customer_id"],
    "left"
).drop(customer_df["customer_id"])

# Rename customer_id to selling_customer_id in the main DataFrame
joined_df = joined_df.withColumnRenamed("customer_id", "selling_customer_id")

# Join for seller location
joined_df = joined_df.join(
    customer_df.select("customer_id", "location_id").withColumnRenamed("location_id", "seller_location_id"),
    joined_df["selling_customer_id"] == customer_df["customer_id"],
    "left"
).drop(customer_df["customer_id"])

# Join for product information
joined_df = joined_df.join(
    product_df.select("product_id", "product_name"),
    joined_df["product_name"] == product_df["product_name"],
    "left"
).drop(product_df["product_name"])

# Join for selling date information
joined_df = joined_df.join(
    date_df.select("date_id", "date").withColumnRenamed("date_id", "selling_date_id"),
    joined_df["selling_date"] == date_df["date"],
    "left"
).drop(date_df["date"])
joined_df = joined_df.withColumn("created_on",lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))  # Add created_on

# Select specific columns from the resulting DataFrame
fact_cust_purchase_df = joined_df.select(
    "id",
    "selling_customer_id",
    "buying_customer_id",
    "seller_location_id",
    "buyer_location_id",
    "selling_date_id",
    "product_id",
    "unit_price",
    "quantity",
    "expected_price",
    "dynamic_price",
    "created_on"
)

# Show the last few rows of the resulting DataFrame
fact_cust_purchase_df.show()
fact_cust_purchase_df.printSchema()

fact_cust_purchase_df.write \
    .format('bigquery') \
    .option('table', f'{project_id}:{dataset_id}.fact_cust_purchase') \
    .mode('overwrite') \
    .save()

# fact_cust_purchase_df.write.mode('overwrite').parquet(f'/home/pce/Documents/VBP_Joint_Project-main/data/exploitation_zone/fact_customer_purchase.parquet')

# Stop the Spark session
spark.stop() 