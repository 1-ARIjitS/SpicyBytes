import logging 
import os 
import configparser
import json
from datetime import datetime
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import udf, monotonically_increasing_id, col, regexp_replace, lit


# Configure logging
logging.basicConfig(level=logging.INFO)  # Set log level to INFO

# Create logger object
logger = logging.getLogger()

# Get base directory
root_dir = os.path.abspath(os.path.join(os.getcwd()))

# Specify the path to config file
config_file_path = os.path.join(root_dir, "config.ini")
config = configparser.ConfigParser()
config.read(config_file_path)

config_file_path_json = os.path.join(root_dir, "config.json")
with open(config_file_path_json) as f:
    config_json = json.load(f)


if __name__ == "__main__":
     # Write data to BigQuery
    project_id = 'formal-atrium-418823'
    dataset_id = 'spicyquery'
    gcs_config = config["GCS"]["credentials_path"]
    raw_bucket_name = config["GCS"]["raw_bucket_name"]
    formatted_bucket_name = config["GCS"]["formatted_bucket_name"]
    exploitation_bucket_name = config["GCS"]["exploitation_bucket_name"]

    spark = SparkSession.builder \
        .appName("Product Dimension table creation") \
        .config("spark.driver.host", "127.0.0.1") \
        .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
        .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
        .config('spark.jars', 'spark-bigquery-with-dependencies_2.12-0.27.0.jar') \
        .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
        .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", gcs_config) \
        .config("temporaryGcsBucket", raw_bucket_name) \
        .config("parentProject", project_id) \
        .config("project", project_id) \
        .getOrCreate()
   
    logger.info('-----------------------------------------------------')
    logger.info("Creating customer dimension table")

     # Read the Parquet file into a DataFrame from GCS Bucket
    customers_df = spark.read.parquet('gs://'+formatted_bucket_name+'/customers*.parquet')
    location_df = spark.read.parquet('gs://'+raw_bucket_name+'/location*.parquet')
    
    cust_location_df = spark.read.parquet('gs://'+raw_bucket_name+'/customer_location*.parquet')
    cust_location_df = cust_location_df.withColumnRenamed("location_id","customer_location").withColumnRenamed("customer_id","customer")

    df1 = customers_df.join(cust_location_df, customers_df.customer_id==cust_location_df.customer,"inner")
    df2 = df1.join(location_df, df1.customer_location==location_df.location_id, "inner")

    dim_cust = df2.select("customer_id", "customer_name", "email_id", "location_id")

    dim_cust = dim_cust.dropDuplicates().withColumn("created_on",lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))

    dim_cust_location = (
        df2
        .select("location_id", "postal_code", "place_name", "latitude", "longitude")
        .groupBy("location_id")
        .agg(
            F.min("postal_code").alias("postal_code"),
            F.min("place_name").alias("place_name"),
            F.min("latitude").alias("latitude"),
            F.min("longitude").alias("longitude")
        )
    ).withColumn("created_on",lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))
    
    
    logger.info("Writng customer dimension table")
    
    dim_cust.write \
    .format('bigquery') \
    .option('table', f'{project_id}:{dataset_id}.dim_customer') \
    .mode('overwrite') \
    .save()
    # .option('temporaryGcsBucket', raw_bucket_name) \
        
    logger.info("writing customer location dimension table")
    
    dim_cust_location.write \
    .format('bigquery') \
    .option('table', f'{project_id}:{dataset_id}.dim_customer_location') \
    .mode('overwrite') \
    .save()