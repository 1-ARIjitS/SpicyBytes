import os
import json
import configparser
import logging
from datetime import datetime, timedelta
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import min, max, explode, sequence, col, date_format, expr, to_date, lit

from pyspark.sql.types import IntegerType

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()

# Get base directory
root_dir = os.path.abspath(os.path.join(os.getcwd()))
print(root_dir)

# Specify the path to config files
config_file_path = os.path.join(root_dir, "config.ini")
config_file_path_json = os.path.join(root_dir, "config.json")

# Load configuration files
config = configparser.ConfigParser()
config.read(config_file_path)

with open(config_file_path_json) as f:
    config_json = json.load(f)

def generate_dates(start_date_str, end_date_str):
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')
    date_list = []
    while start_date <= end_date:
        date_list.append(start_date.strftime('%Y-%m-%d'))
        start_date += timedelta(days=1)
    return date_list


if __name__ == "__main__":
    # Write data to BigQuery
    project_id = 'formal-atrium-418823'
    dataset_id = 'spicyquery'
    gcs_config = config["GCS"]["credentials_path"]
    raw_bucket_name = config["GCS"]["raw_bucket_name"]
    formatted_bucket_name = config["GCS"]["formatted_bucket_name"]
    exploitation_bucket_name = config["GCS"]["exploitation_bucket_name"]

    spark = SparkSession.builder \
        .appName("Date Dimension table creation") \
        .config("spark.driver.host", "127.0.0.1") \
        .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
        .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
        .config('spark.jars', '/home/pce/Documents/VBP_Joint_Project-main/spark-bigquery-with-dependencies_2.12-0.27.0.jar') \
        .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
        .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", gcs_config) \
        .config("temporaryGcsBucket", raw_bucket_name) \
        .config("parentProject", project_id) \
        .config("project", project_id) \
        .getOrCreate()

    logger.info('-----------------------------------------------------')
    logger.info("Creating date dimension table")

    customers_df = spark.read.parquet(f'gs://{raw_bucket_name}/customer_purchase_*')
    min_max_dates = customers_df.agg(
        F.min("purchase_date").alias("min_purchase_date"),
        F.max("purchase_date").alias("max_purchase_date")
    )

    date_df = min_max_dates.select(explode(sequence(to_date(col("min_purchase_date")), to_date(col("max_purchase_date")), expr("INTERVAL 1 DAY")))).alias("date")
    date_df = date_df.withColumn("date_id", F.date_format(F.col("col"), "yyyyMMdd").cast(IntegerType()))
    date_df = date_df.withColumn("day", F.date_format(F.col("col"), "d").cast(IntegerType()))
    date_df = date_df.withColumn("month", F.date_format(F.col("col"), "M").cast(IntegerType()))
    date_df = date_df.withColumn("quarter", ((F.date_format(F.col("col"), "M").cast(IntegerType()) - 1) / 3 + 1).cast(IntegerType()))
    date_df = date_df.withColumn("year", F.date_format(F.col("col"), "yyyy").cast(IntegerType()))
    date_df = date_df.withColumnRenamed("col","date").select("date_id","date","year","month","day")

    # Add created_on
    date_df = date_df.withColumn("created_on",lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))

    date_df.write \
    .format('bigquery') \
    .option('table', f'{project_id}:{dataset_id}.dim_date') \
    .option('temporaryGcsBucket', raw_bucket_name) \
    .mode('overwrite') \
    .save()
    # # # 
    # date_df.printSchema()
    # date_df.show()

    # date_df.write.mode('overwrite').parquet(f'/home/pce/Documents/VBP_Joint_Project-main/dim_table/dim_date.parquet')