from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, trim
from pyspark.sql.types import StringType, FloatType, IntegerType
import random
import datetime

# Initialize the Spark session
spark = SparkSession.builder \
    .appName("supermarket_2_customer_facts") \
    .getOrCreate()

# Load the DataFrames
customer_purchase_path = "data_for_fact/customer_purchase"
supermarket_products_path = "data_for_fact/supermarket_products"
supermarket_location_path = "dim_supermarket.parquet"
products_parquet_path = "dim_product.parquet"
date_parquet_path = "dim_date.parquet"
customer_parquet_path = "dim_customer.parquet"

customer_purchase_df = spark.read.parquet(customer_purchase_path)
supermarket_products_df = spark.read.parquet(supermarket_products_path)
supermarket_location_df = spark.read.parquet(supermarket_location_path)
product_df = spark.read.parquet(products_parquet_path)
date_df = spark.read.parquet(date_parquet_path)
customer_df = spark.read.parquet(customer_parquet_path)

# Filter store_ids to ensure they only include IDs present in supermarket_location_df
valid_store_ids = [row['store_id'] for row in supermarket_products_df.select("store_id").distinct().collect()
                   if row['store_id'] in [row['supermarket_id'] for row in supermarket_location_df.select("supermarket_id").distinct().collect()]]

# Define a UDF to randomly assign a valid store_id
def get_random_store_id():
    return random.choice(valid_store_ids)

random_store_id_udf = udf(get_random_store_id, StringType())

# Define a UDF to generate random purchase dates within the past year
def get_random_purchase_date():
    start_date = datetime.datetime.now() - datetime.timedelta(days=365)
    random_date = start_date + datetime.timedelta(days=random.randint(0, 365))
    return random_date.strftime('%Y-%m-%d')

random_purchase_date_udf = udf(get_random_purchase_date, StringType())

# Define a UDF to generate random quantities between 1 and 10
def get_random_quantity():
    return random.randint(1, 10)

random_quantity_udf = udf(get_random_quantity, IntegerType())

# Define a UDF to calculate dynamic price based on unit_price and proximity to expiry_date
def calculate_dynamic_price(unit_price, purchase_date, expiry_date):
    if unit_price is None or purchase_date is None or expiry_date is None:
        return unit_price
    purchase_date = datetime.datetime.strptime(purchase_date, '%Y-%m-%d')
    expiry_date = datetime.datetime.strptime(expiry_date, '%d %b %Y')
    days_to_expiry = (expiry_date - purchase_date).days
    discount_factor = max(0.1, min(1, days_to_expiry / 365))
    return float(unit_price) * discount_factor

calculate_dynamic_price_udf = udf(calculate_dynamic_price, FloatType())

# Join customer_purchase_df with supermarket_products_df to get expiry_date
joined_df = customer_purchase_df.join(
    supermarket_products_df.select("product_name", "expiry_date"),
    on="product_name",
    how="left"
)

# Add the random_purchase_date, random_quantity, store_id, and dynamic_price columns
joined_df = joined_df.withColumn("purchase_date", random_purchase_date_udf())
joined_df = joined_df.withColumn("quantity", random_quantity_udf())
joined_df = joined_df.withColumn("store_id", random_store_id_udf())
joined_df = joined_df.withColumn("dynamic_price", calculate_dynamic_price_udf(col("unit_price"), col("purchase_date"), col("expiry_date")))

# Ensure both columns have the same data type, remove leading/trailing spaces, and clean hidden characters
clean_store_id_udf = udf(lambda x: ''.join(c for c in x if c.isprintable()), StringType())
joined_df = joined_df.withColumn("store_id", trim(clean_store_id_udf(col("store_id"))).cast("string"))
supermarket_location_df = supermarket_location_df.withColumn("supermarket_id", trim(clean_store_id_udf(col("supermarket_id"))).cast("string"))

# Display some samples to debug
print("Sample store_ids from joined_df:")
joined_df.select("store_id").distinct().show(5, truncate=False)

print("Sample supermarket_ids from supermarket_location_df:")
supermarket_location_df.select("supermarket_id").distinct().show(5, truncate=False)

# Perform an inner join to check for matches
matches_df = joined_df.join(
    supermarket_location_df.select("supermarket_id"),
    joined_df["store_id"] == supermarket_location_df["supermarket_id"],
    "inner"
)

# Show the number of matches
match_count = matches_df.count()
print(f"Number of matches: {match_count}")

# Display some matching rows if there are any
if match_count > 0:
    matches_df.show()
else:
    print("No matches found.")

# Join with supermarket_location_df to get location_id
joined_df = joined_df.join(
    supermarket_location_df.select("supermarket_id", "location_id"),
    joined_df["store_id"] == supermarket_location_df["supermarket_id"],
    "left"
)

# Join for product information
joined_df = joined_df.join(
    product_df.select("product_id", "product_name"),
    joined_df["product_name"] == product_df["product_name"],
    "left"
).drop(product_df["product_name"])

# Join for selling date information
joined_df = joined_df.join(
    date_df.select("date_id", "date").withColumnRenamed("date_id", "purchase_date_id"),
    joined_df["purchase_date"] == date_df["date"],
    "left"
).drop(date_df["date"])

# Join for buyer location
joined_df = joined_df.join(
    customer_df.select("customer_id", "location_id").withColumnRenamed("location_id", "buyer_location_id"),
    joined_df["customer_id"] == customer_df["customer_id"],
    "left"
).drop(customer_df["customer_id"])

# Select the required columns
selected_columns_df = joined_df.select(
    "store_id",
    "customer_id",
    "location_id",
    "buyer_location_id",
    "product_id",
    "purchase_date_id",
    "unit_price",
    "quantity",
    "dynamic_price"
)

# Show the resulting DataFrame
selected_columns_df.show()

# Select rows where location_id is not null
non_null_location_df = selected_columns_df.filter(col("location_id").isNotNull())

# Show the resulting DataFrame
non_null_location_df.show()

# Write the resulting DataFrame to a new parquet file
output_parquet_path = "customer_purchase_with_store_id_random_quantity_and_dynamic_price.parquet"
selected_columns_df.write.mode("overwrite").parquet(output_parquet_path)

# Stop the Spark session
spark.stop()
