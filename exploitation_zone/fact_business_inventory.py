import logging 
import os 
import configparser
import json
from pyspark.sql import SparkSession
from datetime import datetime
from pyspark.sql.functions import udf, monotonically_increasing_id, col, regexp_replace, lit, to_date
from pyspark.sql.functions import row_number
from pyspark.sql.window import Window

# Configure logging
logging.basicConfig(level=logging.INFO)  # Set log level to INFO

# Create logger object
logger = logging.getLogger()

# Get base directory
root_dir = os.path.abspath(os.path.join(os.getcwd()))

# Specify the path to config file
config_file_path = os.path.join(root_dir, "config.ini")
config = configparser.ConfigParser()
config.read(config_file_path)

config_file_path_json = os.path.join(root_dir, "config.json")
with open(config_file_path_json) as f:
    config_json = json.load(f)


if __name__ == "__main__":
    gcs_config = config["GCS"]["credentials_path"]
    raw_bucket_name = config["GCS"]["raw_bucket_name"]
    formatted_bucket_name = config["GCS"]["formatted_bucket_name"]
    exploitation_bucket_name = config["GCS"]["exploitation_bucket_name"]
    project_id = config["BIGQUERY"]["project_id"]
    dataset_id = config["BIGQUERY"]["dataset_id"]

    spark = SparkSession.builder \
        .appName("Supermarket Fact table creation") \
        .config("spark.driver.host", "127.0.0.1") \
        .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
        .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
        .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
        .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", gcs_config) \
        .getOrCreate()
   
    logger.info('-----------------------------------------------------')
    logger.info("Creating business inventory fact table")

    exploitation_zone_parquet_file_path = os.path.join(root_dir, 'data', 'exploitation_zone')

    # Read the Parquet file into a DataFrame from GCS Bucket
    supermarket_inventory_df = spark.read.parquet('gs://'+formatted_bucket_name+'/supermarket_inventory*.parquet')  # note that I'm using this and not supermarket_products.


    dim_product_df = spark.read.parquet(os.path.join(exploitation_zone_parquet_file_path, 'dim_product.parquet'))
    dim_supermarket_df = spark.read.parquet(os.path.join(exploitation_zone_parquet_file_path, 'dim_supermarket.parquet'))

    # supermarket_inventory_df.show()
    # dim_product_df.show()
    # dim_supermarket_df.show()

    supermarket_inventory_df = supermarket_inventory_df.withColumnRenamed("store_id", "supermarket_id")
    fact_business_inventory_df = supermarket_inventory_df.join(dim_supermarket_df, 'supermarket_id', 'inner').select(supermarket_inventory_df['*'], dim_supermarket_df['location_id'])

    fact_business_inventory_df = fact_business_inventory_df.drop("product_id") # removing product_id column because it should come from dimension product

    fact_business_inventory_df = fact_business_inventory_df.join(dim_product_df, 'product_name', 'inner').select(fact_business_inventory_df['*'], dim_product_df['product_id'])
    fact_business_inventory_df = fact_business_inventory_df.withColumn("created_on",lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))  # Add created_on

    # creating static expiry date id for now
    fact_business_inventory_df = fact_business_inventory_df.withColumn("expiry_date_id", lit('20240524'))

    window_spec = Window.orderBy("product_name")
    fact_business_inventory_df = fact_business_inventory_df.withColumn("supermarket_inventory_id", row_number().over(window_spec))

    fact_business_inventory_df = fact_business_inventory_df.select("supermarket_inventory_id", "supermarket_id", "location_id", "product_id", "price", "quantity", "expiry_date_id", "created_on")
    # Note : Expiry Date ID is missing since its not in the data yet.

    # fact_business_inventory_df.show()

    fact_business_inventory_df.printSchema()

    fact_business_inventory_df.write \
    .format('bigquery') \
    .option('table', f'{project_id}:{dataset_id}.fact_business_inventory') \
    .option('temporaryGcsBucket', raw_bucket_name) \
    .mode('overwrite') \
    .save()

    fact_business_inventory_df.write.mode('overwrite').parquet(os.path.join(exploitation_zone_parquet_file_path, 'fact_business_inventory.parquet'))

